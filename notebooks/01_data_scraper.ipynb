{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600daea-e35f-4ba7-811b-f63f8214ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import unicodecsv\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas import DataFrame, read_csv, concat, set_option, to_datetime\n",
    "import urllib\n",
    "from typing import List, Dict, Tuple\n",
    "from re import search, match, compile, sub\n",
    "import os\n",
    "import logging\n",
    "import zipfile\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8c606-2f1b-4fd1-b0b1-667ff695e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option('display.max_rows', 1000)\n",
    "set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb15c4-64c6-4e1e-9abf-579a0dde4133",
   "metadata": {},
   "source": [
    "# Historical rides data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9180f5-da65-4523-9a96-542cb1797da2",
   "metadata": {},
   "source": [
    "Historical rides data is made available through bixi's site https://bixi.com/en/open-data/. The underlying html strucutre of the page contains 1 s3 endpoint for each year of data. The following is an example of the aforementioned endpoint:\n",
    "- https://s3.ca-central-1.amazonaws.com/cdn.bixi.com/wp-content/uploads/2024/03/DonneesOuvertes2024_010203.zip\n",
    "\n",
    "In order to reliably access this information, an ETL pipeline object will be designed in the next steps. See below the pseudo code and context for each one of the ETL steps. In addition, see below the desired schema for this dataset:\n",
    "\n",
    "     - start_station_name : str\n",
    "     - start_station_latitude: float\n",
    "     - start_station_longitude: float\n",
    "     - end_station_name : str\n",
    "     - end_station_latitude: float\n",
    "     - end_station_longitude: float\n",
    "     - start_date: datetime[ns]\n",
    "     - end_date: datetime[ns]\n",
    "\n",
    "**Extract**:\n",
    "\n",
    "Ride data files need to be extracted from bixi's website and persisted on a temporary folder. The following lays out the logic for the extract step:\n",
    "- Turn bixi's website into `BeautifulSoup` object using `BeautifulSoup(webpage.content)` logic.\n",
    "- Find all the s3 endpoints and select the endpoints dor which there is not yet data.\n",
    "- For the selected endpoints, use the `requests` library to get the underlying zip file(s)\n",
    "- Open the zip file and save it on the designated folder structure based on the year.\n",
    "\n",
    "**Transform**:\n",
    "\n",
    "Once the rides data zip files are extracted, data needs to be transformed and compiled into a dataset for all years for which data is available. The preliminary exploration of the rides data allowed the idenfications of two distinct schema: before and after the year 2022. The following lays out the logic for the transform step:\n",
    "- Retrive the name of all files contained inside the zip file using method `zipfile.ZipFile('<file_name>').names_list()`.\n",
    "- If the data dates from 2021 or earlier:\n",
    "    - open and concatanted all `.csv` excluding the file containing the regex pattern `.*[s-S]atations.*`\n",
    "    - open the file containing regex pattern `.*[s-S]atations.*` and merge it with the concatened files from previous step in order to link denormalize the stations names. The fields for the merge vary depending on the year which will be hardcoded.\n",
    "    - confrom data to the desired schema           \n",
    "- Else:\n",
    "    -  open the `.csv` file and confrom to the desired schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d558ef4-fe3b-479f-b42a-016320dc0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b38937-d3c6-4b72-af9a-2151e5c6e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidesDataPipeline:\n",
    "    \"\"\"\n",
    "    Extract, load and transform pipeline object for bixi rides data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        url: str, \n",
    "        temporary_directory: str, \n",
    "        years: Tuple[int, int], \n",
    "        column_name_mapping: Dict[str, str], \n",
    "        ride_data_columns: List[str]\n",
    "    ):\n",
    "        self.url = url\n",
    "        self.raw_data_directory = temporary_directory\n",
    "        self.years = [y for y in range(years[0], years[1] +1)]\n",
    "        self.column_name_mapping = column_name_mapping\n",
    "        self.ride_data_columns = ride_data_columns\n",
    "\n",
    "    def _get_s3_endpoints(self) -> Dict[int, str]:\n",
    "        \"\"\"Webscrape and extract all available s3 endpoints metadata from bixi's website\"\"\"\n",
    "        \n",
    "        response = requests.get(self.url, 'html.parser')\n",
    "        soup_classes = (\n",
    "            BeautifulSoup(response.content)\n",
    "            .find_all('a', href=True, attrs={'class':'button button-primary'})\n",
    "        )\n",
    "        s3_endpoints = [s3['href'] for s3 in soup_classes if 's3.ca' in s3['href']]\n",
    "\n",
    "        s3_endpoint_by_year = {\n",
    "            int(search('2[0-9]{3}', endpoint.split('/')[-1])[0]) : endpoint # Get only endpoints with year in the string\n",
    "            for endpoint in s3_endpoints\n",
    "        }\n",
    "         \n",
    "        return s3_endpoint_by_year\n",
    "\n",
    "    def _save_rides_data_files(self, s3_endpoint_by_year: Dict) -> None:\n",
    "        \"\"\"Extracts and save rides data das zip files\"\"\"\n",
    "        \n",
    "        for year in self.years:\n",
    "            response = requests.get(s3_endpoint_by_year[year])\n",
    "            file_name = f\"{self.raw_data_directory}/raw_data/rides_data/{year}/{year}_hist_rides_data.zip\"\n",
    "            os.makedirs(os.path.dirname(file_name), exist_ok=True) \n",
    "            \n",
    "            with open(file_name, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            f.close()\n",
    "            logging.info(f'Data for year {year} saved successfully.')\n",
    "            \n",
    "        return\n",
    "\n",
    "    def _retrive_csv_files_path(self) -> Dict[int, List[str]]:\n",
    "        \"\"\"Retrive csv files path from the zip files extracted\"\"\"\n",
    "    \n",
    "        csv_files_path  = {}\n",
    "        for year in self.years:\n",
    "            zip_file_object = zipfile.ZipFile(\n",
    "                f\"{self.raw_data_directory}/raw_data/rides_data/{year}/{year}_hist_rides_data.zip\"\n",
    "            )\n",
    "            csv_files_path[year] = [file for file in zip_file_object.namelist() if search(\".*csv*\", file)]\n",
    "            \n",
    "        return csv_files_path\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _standarize_columns(df: DataFrame, column_name_mapping: Dict[str, str], year: int) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Standarize columns by lowercasing, renaming and enforcing data types based on year selected\n",
    "        \"\"\"\n",
    "\n",
    "        df.columns = [col.lower().strip() for col in df.columns]\n",
    "        standard_df = df.rename(columns=column_name_mapping)\n",
    "        \n",
    "        if all(['start_date' in standard_df, year >= 2022]):\n",
    "            standard_df = standard_df.assign(\n",
    "                start_date=to_datetime(standard_df['start_date'], unit='ms'),\n",
    "                end_date=to_datetime(standard_df['end_date'], unit='ms')\n",
    "            )\n",
    "            \n",
    "        elif all(['start_date' in standard_df, year < 2022]):\n",
    "            standard_df = standard_df.assign(\n",
    "                start_date=to_datetime(standard_df['start_date']),\n",
    "                end_date=to_datetime(standard_df['end_date'])\n",
    "            )\n",
    "            \n",
    "        \n",
    "        return standard_df\n",
    "            \n",
    "\n",
    "    def extract(self) -> None:\n",
    "        \"\"\"Extracts and saves rides data available through bixi S3 endpoints\"\"\"\n",
    "        \n",
    "        try:\n",
    "            s3_endpoints = self._get_s3_endpoints()\n",
    "\n",
    "            logging.info(f'The following s3 endpoints were detected: {s3_endpoints}')\n",
    "            \n",
    "            self._save_rides_data_files(\n",
    "                s3_endpoints\n",
    "            )\n",
    "            \n",
    "            logging.info(f'Data for endpoints {s3_endpoints} was sucessfully extracted and saved.')\n",
    "                     \n",
    "            return \n",
    "           \n",
    "        except Exception as e:\n",
    "            logging.error(f'Extraction was not successfull given the following excpetion: {e}')\n",
    "                     \n",
    "            return \n",
    "            \n",
    "    def transform(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Combine rides data into a single dataframe with standard columns names and schema\n",
    "        \"\"\"\n",
    "    \n",
    "        csv_paths = self._retrive_csv_files_path()\n",
    "        ride_data_columns = [\n",
    "            'start_date',\n",
    "            'end_date',\n",
    "            'name_start_station',\n",
    "            'name_end_station'\n",
    "        ]\n",
    "\n",
    "        dfs = []\n",
    "        for year in self.years:\n",
    "            logging.info(f'Transforming data for year {year}.')\n",
    "            \n",
    "            zip_file_object = zipfile.ZipFile(\n",
    "                f\"{RAW_DATA_DIRECTORY}/raw_data/rides_data/{year}/{year}_hist_rides_data.zip\"\n",
    "            )\n",
    "            \n",
    "            stations_file_path = next(\n",
    "                (file for file in csv_paths[year] if search('.*[sS]tations.*', file)),\n",
    "                None\n",
    "            )\n",
    "\n",
    "            if year < 2022:\n",
    "                with zip_file_object.open(f'{stations_file_path}', 'r') as file:\n",
    "                    stations_dataframe = read_csv(file)\n",
    "            else:\n",
    "                stations_dataframe = DataFrame()\n",
    "            \n",
    "            rides_files_path = [\n",
    "                file for file in csv_paths[year] if file != stations_file_path\n",
    "            ]\n",
    "            \n",
    "            rides_dataframes_list = []\n",
    "            for file_path in rides_files_path:\n",
    "                with zip_file_object.open(f'{file_path}', 'r') as file:\n",
    "                    rides_dataframes_list.append(read_csv(file))\n",
    "                    \n",
    "            rides_dataframe = concat(rides_dataframes_list)\n",
    "            \n",
    "            rides_dataframe = self._standarize_columns(rides_dataframe, self.column_name_mapping, year)\n",
    "            stations_dataframe = self._standarize_columns(stations_dataframe, self.column_name_mapping, year)\n",
    "                   \n",
    "            if year < 2022:\n",
    "                rides_dataframe_denormalized = (\n",
    "                    rides_dataframe.merge(\n",
    "                        stations_dataframe,\n",
    "                        how='left',\n",
    "                        left_on='code_start_station',\n",
    "                        right_on='code'\n",
    "                    )\n",
    "                    .merge(\n",
    "                        stations_dataframe,\n",
    "                        how='left',\n",
    "                        left_on='code_end_station',\n",
    "                        right_on='code',\n",
    "                        suffixes=('_rides','_station')\n",
    "                    )\n",
    "                    .drop(['is_member','duration_sec'], axis=1)\n",
    "                    .rename(\n",
    "                        columns={\n",
    "                            'latitude_rides':'latitude_start_station',\n",
    "                            'longitude_rides':'longitude_start_station',\n",
    "                            'latitude_station' :'latitude_end_station',\n",
    "                            'longitude_station' :'longitude_end_station',\n",
    "                            'name_rides':'name_start_station',\n",
    "                            'name_station':'name_end_station',\n",
    "                            'code_rides' :'code_start_station',\n",
    "                            'code_station':'code_end_station'\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                dfs.append(rides_dataframe_denormalized[self.ride_data_columns])\n",
    "            \n",
    "            else:\n",
    "                dfs.append(rides_dataframe[self.ride_data_columns])\n",
    "        \n",
    "        return concat(dfs)\n",
    "\n",
    "    def load(self, df: DataFrame, target_directory: str='', file_name: str='historical_rides_data') -> None:\n",
    "        \"\"\"\n",
    "        Load transformed data into an specific target_directory\n",
    "        \"\"\"\n",
    "        \n",
    "        df.to_csv(f'{target_directory}{file_name}.csv')\n",
    "        logging.info(f'Rides data sucessfully loaded on {target_directory}{file_name}.csv')\n",
    "        \n",
    "    def run_pipeline(self, steps: List[str]=['extract','transform','load'], **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Run entire RidesDataPipeline consisting of steps extract, transform and load\n",
    "        \"\"\"\n",
    "        if 'extract' in steps:\n",
    "            self.extract()\n",
    "        elif 'transform' in steps:\n",
    "            df = self.transform()\n",
    "        elif 'load' in steps:\n",
    "            self.load(df, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f0f5c-b42c-4303-8402-8f6b2c51a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "BIXI_WEBSITE = \"https://bixi.com/en/open-data/\"\n",
    "RAW_DATA_DIRECTORY = \"/mnt/c/Users/felip/Documents/knowledge/ml_projects/protifolio/bixi_ride_predictions/data\"\n",
    "COLUMN_MAPPING = {\n",
    "    'pk':'code',\n",
    "    'emplacement_pk_start': 'code_start_station',\n",
    "    'emplacement_pk_end': 'code_end_station',\n",
    "    'startstationname': 'name_start_station', \n",
    "    'startstationarrondissement':'arrondissement_start_station',\n",
    "    'startstationlatitude':'latitude_start_station', \n",
    "    'startstationlongitude':'longitude_start_station',\n",
    "    'endstationname': 'name_end_station',\n",
    "    'endstationarrondissement':'arrondissement_end_station', \n",
    "    'endstationlatitude':'latitude_end_station', \n",
    "    'endstationlongitude':'longitude_end_station',\n",
    "    'starttimems':'start_date', \n",
    "    'endtimems': 'end_date',\n",
    "    \"start_station_code\":\"code_start_station\",\n",
    "    \"end_station_code\":\"code_end_station\"\n",
    "}\n",
    "\n",
    "RIDE_DATA_COLUMNS = [\n",
    "    'start_date',\n",
    "    'end_date',\n",
    "    'name_start_station',\n",
    "    'name_end_station',\n",
    "    'latitude_start_station',\n",
    "    'longitude_start_station',\n",
    "    'latitude_end_station',\n",
    "    'longitude_end_station'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db142a9f-9faa-44f2-a76a-875d0295e83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs_ = RidesDataPipeline(\n",
    "    'https://bixi.com/en/open-data/', \n",
    "    RAW_DATA_DIRECTORY,\n",
    "    (2022, 2024),\n",
    "    COLUMN_MAPPING,\n",
    "    RIDE_DATA_COLUMNS,\n",
    ").run_pipeline(file_name='historical_rides_data_2022_to_2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696d9ae-a605-4f89-85b7-ec2c8487c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050890bb-b6bd-4291-8160-bb1973146b6d",
   "metadata": {},
   "source": [
    "# Historical data weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf29673-3573-47e6-9682-4fae6a1a76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_hourly_weather_data(stationID: int, year: str, month: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads and transfrom historical data based on the data parameters into a dataframe\n",
    "    \"\"\"\n",
    "    base_url = 'http://climate.weather.gc.ca/climate_data/bulk_data_e.html?'\n",
    "    query_url = 'format=csv&stationID={}&Year={}&Month={}&timeframe=1'.format(stationID, year, month)\n",
    "    api_endpoint = base_url + query_url\n",
    "    return read_csv(api_endpoint, skiprows=0)\n",
    "\n",
    "# List of months and years to iterate with\n",
    "years = [ y for y in range(2014, 2024)]\n",
    "months = [ f'0{m}' if m < 10 else f'{m}' for m in range(1, 13)]\n",
    "\n",
    "weather_data = concat([\n",
    "    get_hourly_weather_data(30165, year, month)\n",
    "    for year in years\n",
    "    for month in months\n",
    "])\n",
    "\n",
    "weather_data.to_csv('/mnt/c/Users/felip/Documents/knowledge/ml_projects/protifolio/bixi_ride_predictions/data/hist_weather_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
