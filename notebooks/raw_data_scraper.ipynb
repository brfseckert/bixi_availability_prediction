{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f0fca-ddaa-40cb-bfde-3ef516e6a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import dotenv\n",
    "import duckdb\n",
    "import git\n",
    "from google.cloud import storage\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Part 1: listing existing archives\n",
    "\n",
    "# The first thing we do is list the blobs in the bucket. This way, we know which data has already\n",
    "# been archived, and which data we need to archive.\n",
    "\n",
    "service_account_info = json.loads(os.environ[\"GCP_SERVICE_ACCOUNT_JSON\"], strict=False)\n",
    "client = storage.Client.from_service_account_info(\n",
    "    service_account_info, project=\"bike-sharing\"\n",
    ")\n",
    "\n",
    "# Parse blob names like paris/smovengo/2023/Nov.parquet\n",
    "existing_parquet_keys = {\n",
    "    (\n",
    "        \"bike-sharing\",\n",
    "        *re.match(r\"([\\w\\-]+)/([\\w\\-]+)/(\\d+)/(\\w+)\\.parquet\", blob.name).groups(),\n",
    "    )\n",
    "    for blob in client.get_bucket(\"bike-sharing-history\").list_blobs()\n",
    "} | {\n",
    "    (\n",
    "        \"weather-forecast\",\n",
    "        *re.match(r\"([\\w\\-]+)/(\\d+)/(\\w+)\\.parquet\", blob.name).groups(),\n",
    "    )\n",
    "    for blob in client.get_bucket(\"weather-forecast-history\").list_blobs()\n",
    "}\n",
    "print(f\"{len(existing_parquet_keys):,d} existing parquet files\")\n",
    "\n",
    "# Part 2: archiving the data into local CSV files\n",
    "\n",
    "\n",
    "def jcdecaux_scrub(geojson):\n",
    "    for station in geojson[\"features\"]:\n",
    "        yield {\n",
    "            \"station\": station[\"properties\"][\"name\"],\n",
    "            \"longitude\": station[\"geometry\"][\"coordinates\"][0],\n",
    "            \"latitude\": station[\"geometry\"][\"coordinates\"][1],\n",
    "            \"bikes\": station[\"properties\"][\"available_bikes\"],\n",
    "            \"stands\": station[\"properties\"][\"available_bike_stands\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def gbfs_scrub(geojson):\n",
    "    for station in geojson[\"features\"]:\n",
    "        yield {\n",
    "            \"station\": station[\"properties\"][\"name\"],\n",
    "            \"longitude\": station[\"geometry\"][\"coordinates\"][0],\n",
    "            \"latitude\": station[\"geometry\"][\"coordinates\"][1],\n",
    "            \"bikes\": station[\"properties\"][\"num_bikes_available\"],\n",
    "            \"stands\": station[\"properties\"][\"num_docks_available\"],\n",
    "        }\n",
    "\n",
    "\n",
    "# List the existing (city, provider) pairs\n",
    "systems = [\n",
    "    (city.stem, provider.stem)\n",
    "    for city in (pathlib.Path(\"data\") / \"stations\").iterdir()\n",
    "    for provider in city.iterdir()\n",
    "    if provider.stem not in {\"lime\"}\n",
    "]\n",
    "\n",
    "# We're going to loop over all the commits since the start of time. It's ok to do this, because\n",
    "# most commits will be skipped. Indeed, we can skip all commits which pertain to a month which\n",
    "# has already been archived.\n",
    "# We do however limit ourselves to the latest full month, because we don't want to archive partial\n",
    "# months.\n",
    "end_of_last_month = dt.datetime.now(dt.timezone.utc).replace(\n",
    "    day=1, hour=23, minute=59, second=59\n",
    ") - dt.timedelta(days=1)\n",
    "commits = git.Repo(\".\").iter_commits(\"--all\", reverse=True, until=end_of_last_month)\n",
    "archive_dir = pathlib.Path(\"archive\")\n",
    "\n",
    "# We keep track of the latest update for each station, so that we can skip updates which don't\n",
    "# change anything. We also keep track of the number of skipped updates, so that we can record\n",
    "# this information in the CSV file.\n",
    "latest_update_by_station = collections.defaultdict()\n",
    "skipped_updates_by_station = collections.defaultdict(int)\n",
    "\n",
    "for i, commit in enumerate(commits):\n",
    "    commit_at = commit.committed_datetime.astimezone(dt.timezone.utc)\n",
    "    year = commit_at.strftime(\"%Y\")\n",
    "    month = commit_at.strftime(\"%h\")\n",
    "\n",
    "    if i % 1_000 == 0:\n",
    "        print(f\"Processing commit {i} ({commit_at.isoformat()})\")\n",
    "\n",
    "    # This loop is responsible for archiving the bike station updates. The next loop will archive\n",
    "    # the weather updates.\n",
    "    for city, provider in systems:\n",
    "        # Skip if the data has already been stored\n",
    "        if (\"bike-sharing\", city, provider, str(year), month) in existing_parquet_keys:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            blob = commit.tree / \"data\" / \"stations\" / city / f\"{provider}.geojson\"\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # The data has been scrapped and stored as is. This is where we normalize it to a single\n",
    "        # format.\n",
    "        scrub = {\n",
    "            \"jcdecaux\": jcdecaux_scrub,\n",
    "        }.get(provider, gbfs_scrub)\n",
    "\n",
    "        # We store the data in a CSV file per month\n",
    "        csv_file = archive_dir / \"stations\" / city / provider / year / f\"{month}.csv\"\n",
    "        csv_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(csv_file, \"a\") as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\n",
    "                    \"station\",\n",
    "                    \"longitude\",\n",
    "                    \"latitude\",\n",
    "                    \"commit_at\",\n",
    "                    \"skipped_updates\",\n",
    "                    \"bikes\",\n",
    "                    \"stands\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # Write the header if the file is empty\n",
    "            if csv_file.stat().st_size == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            for update in scrub(geojson=json.load(blob.data_stream)):\n",
    "                station_key = (city, provider, update[\"station\"])\n",
    "\n",
    "                # We don't write anything if the data hasn't changed\n",
    "                if (\n",
    "                    (latest_update := latest_update_by_station.get(station_key))\n",
    "                    and latest_update[\"bikes\"] == update[\"bikes\"]\n",
    "                    and latest_update[\"stands\"] == update[\"stands\"]\n",
    "                ):\n",
    "                    skipped_updates_by_station[station_key] += 1\n",
    "                    continue\n",
    "\n",
    "                update[\"commit_at\"] = commit_at.isoformat()\n",
    "                update[\"skipped_updates\"] = skipped_updates_by_station[station_key]\n",
    "                writer.writerow(update)\n",
    "                latest_update_by_station[station_key] = update\n",
    "                skipped_updates_by_station[station_key] = 0\n",
    "\n",
    "    # This loop is responsible for archiving the weather updates.\n",
    "    for city in {city for city, _ in systems}:\n",
    "        # Skip if the data has already been stored\n",
    "        if (\"weather-forecast\", city, str(year), month) in existing_parquet_keys:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            blob = commit.tree / \"data\" / \"weather\" / f\"{city}.json\"\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # We store the data in a CSV file per month\n",
    "        csv_file = archive_dir / \"weather\" / city / year / f\"{month}.csv\"\n",
    "        csv_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(csv_file, \"a\") as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\n",
    "                    \"commit_at\",\n",
    "                    \"forecast_at\",\n",
    "                    \"temperature\",\n",
    "                    \"rain\",\n",
    "                    \"wind_speed\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # Write the header if the file is empty\n",
    "            if csv_file.stat().st_size == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # There is one row per forecast time step to write\n",
    "            try:\n",
    "                forecast = json.load(blob.data_stream)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding {blob}\")\n",
    "                continue\n",
    "            for forecast_at, temperature, rain, wind_speed in zip(\n",
    "                forecast[\"hourly\"][\"time\"],\n",
    "                forecast[\"hourly\"][\"temperature_2m\"],\n",
    "                forecast[\"hourly\"][\"rain\"],\n",
    "                forecast[\"hourly\"][\"windspeed_10m\"],\n",
    "            ):\n",
    "                writer.writerow(\n",
    "                    {\n",
    "                        \"commit_at\": commit_at.isoformat(),\n",
    "                        \"forecast_at\": forecast_at,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"rain\": rain,\n",
    "                        \"wind_speed\": wind_speed,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "# Part 3: uploading the CSV files to GCS after converting them to Parquet\n",
    "\n",
    "for csv_file in archive_dir.rglob(\"stations/**/*.csv\"):\n",
    "    # Let's skip the files which are empty, ignoring the header\n",
    "    with open(csv_file) as f:\n",
    "        for i, _ in enumerate(f):\n",
    "            if i > 0:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Skip if the data has already been stored\n",
    "    key = (\n",
    "        \"bike-sharing\",\n",
    "        *re.match(\n",
    "            r\"archive/stations/([\\w\\-]+)/([\\w\\-]+)/(\\d+)/(\\w+).csv\", str(csv_file)\n",
    "        ).groups(),\n",
    "    )\n",
    "    if key in existing_parquet_keys:\n",
    "        print(\"Skipping\", \"/\".join(key))\n",
    "        continue\n",
    "\n",
    "    parquet_file = csv_file.with_suffix(\".parquet\")\n",
    "    duckdb.connect(\":memory:\").execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM read_csv('{csv_file}', AUTO_DETECT=TRUE)\n",
    "    )\n",
    "    TO '{parquet_file}' (FORMAT 'PARQUET', CODEC 'ZSTD');\n",
    "    \"\"\")\n",
    "\n",
    "    bucket = client.get_bucket(\"bike-sharing-history\")\n",
    "    blob = bucket.blob(str(parquet_file.relative_to(archive_dir / \"stations\")))\n",
    "    blob.upload_from_filename(str(parquet_file), timeout=60 * 10)\n",
    "    print(f\"Uploaded {parquet_file} to {blob.name}\")\n",
    "\n",
    "for csv_file in archive_dir.rglob(\"weather/**/*.csv\"):\n",
    "    parquet_file = csv_file.with_suffix(\".parquet\")\n",
    "\n",
    "    # Skip if the data has already been stored\n",
    "    key = (\n",
    "        \"weather-forecast\",\n",
    "        *re.match(r\"archive/weather/([\\w\\-]+)/(\\d+)/(\\w+).csv\", str(csv_file)).groups(),\n",
    "    )\n",
    "    if key in existing_parquet_keys:\n",
    "        print(\"Skipping\", \"/\".join(key))\n",
    "        continue\n",
    "\n",
    "    duckdb.connect(\":memory:\").execute(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM read_csv('{csv_file}', AUTO_DETECT=TRUE)\n",
    "    )\n",
    "    TO '{parquet_file}' (FORMAT 'PARQUET', CODEC 'ZSTD');\n",
    "    \"\"\")\n",
    "\n",
    "    bucket = client.get_bucket(\"weather-forecast-history\")\n",
    "    blob = bucket.blob(str(parquet_file.relative_to(archive_dir / \"weather\")))\n",
    "    blob.upload_from_filename(str(parquet_file), timeout=60 * 10)\n",
    "    print(f\"Uploaded {parquet_file} to {blob.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efb34fb-8c42-4457-81a0-7fd44c94c5d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'github'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgit\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgithub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Github  \u001b[38;5;66;03m# Install this with pip install PyGithub\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjcdecaux_scrub\u001b[39m(geojson):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m station \u001b[38;5;129;01min\u001b[39;00m geojson[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'github'"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import collections\n",
    "import pathlib\n",
    "import json\n",
    "import csv\n",
    "import git\n",
    "from github import Github  # Install this with pip install PyGithub\n",
    "\n",
    "def jcdecaux_scrub(geojson):\n",
    "    for station in geojson[\"features\"]:\n",
    "        yield {\n",
    "            \"station\": station[\"properties\"][\"name\"],\n",
    "            \"longitude\": station[\"geometry\"][\"coordinates\"][0],\n",
    "            \"latitude\": station[\"geometry\"][\"coordinates\"][1],\n",
    "            \"bikes\": station[\"properties\"][\"available_bikes\"],\n",
    "            \"stands\": station[\"properties\"][\"available_bike_stands\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def gbfs_scrub(geojson):\n",
    "    for station in geojson[\"features\"]:\n",
    "        yield {\n",
    "            \"station\": station[\"properties\"][\"name\"],\n",
    "            \"longitude\": station[\"geometry\"][\"coordinates\"][0],\n",
    "            \"latitude\": station[\"geometry\"][\"coordinates\"][1],\n",
    "            \"bikes\": station[\"properties\"][\"num_bikes_available\"],\n",
    "            \"stands\": station[\"properties\"][\"num_docks_available\"],\n",
    "        }\n",
    "\n",
    "\n",
    "# Set up the end of the last month\n",
    "end_of_last_month = dt.datetime.now(dt.timezone.utc).replace(\n",
    "    day=1, hour=23, minute=59, second=59\n",
    ") - dt.timedelta(days=1)\n",
    "\n",
    "# Initialize repository and iterate through commits\n",
    "repo = git.Repo(\"/path/to/bike-sharing-history\")  # Make sure to specify the correct repo path\n",
    "commits = repo.iter_commits(\"--all\", reverse=True, until=end_of_last_month)\n",
    "\n",
    "archive_dir = pathlib.Path(\"archive\")\n",
    "\n",
    "# To keep track of the latest updates for each station\n",
    "latest_update_by_station = collections.defaultdict()\n",
    "skipped_updates_by_station = collections.defaultdict(int)\n",
    "\n",
    "# We will assume `systems` is a predefined list of bike-sharing systems\n",
    "# Example format: systems = [(\"city_name\", \"provider_name\"), ...]\n",
    "systems = [\n",
    "    (\"montreal\", \"bixi\")\n",
    "    # Add more cities and providers as needed\n",
    "]\n",
    "\n",
    "# Initialize Github API to reference the remote repository if needed\n",
    "g = Github(\"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\")  # Replace with your GitHub token if necessary\n",
    "repo_url = \"MaxHalford/bike-sharing-history\"\n",
    "repository = g.get_repo(repo_url)\n",
    "\n",
    "# Loop through the commits\n",
    "for i, commit in enumerate(commits):\n",
    "    commit_at = commit.committed_datetime.astimezone(dt.timezone.utc)\n",
    "    year = commit_at.strftime(\"%Y\")\n",
    "    month = commit_at.strftime(\"%h\")\n",
    "\n",
    "    if i % 1_000 == 0:\n",
    "        print(f\"Processing commit {i} ({commit_at.isoformat()})\")\n",
    "\n",
    "    # Archive bike station updates\n",
    "    for city, provider in systems:\n",
    "        # Skip if the data has already been archived\n",
    "        if (\"bike-sharing\", city, provider, str(year), month) in existing_parquet_keys:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Modify this path based on the actual data structure of the repository\n",
    "            blob = commit.tree / \"data\" / \"stations\" / city / f\"{provider}.geojson\"\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # Data normalization function (you might want to adjust based on the provider)\n",
    "        scrub = {\n",
    "            \"jcdecaux\": jcdecaux_scrub,\n",
    "        }.get(provider, gbfs_scrub)\n",
    "\n",
    "        # Archive data into CSV file by month\n",
    "        csv_file = archive_dir / \"stations\" / city / provider / year / f\"{month}.csv\"\n",
    "        csv_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(csv_file, \"a\") as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f,\n",
    "                fieldnames=[\n",
    "                    \"station\",\n",
    "                    \"longitude\",\n",
    "                    \"latitude\",\n",
    "                    \"commit_at\",\n",
    "                    \"skipped_updates\",\n",
    "                    \"bikes\",\n",
    "                    \"stands\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # Write the header if the file is empty\n",
    "            if csv_file.stat().st_size == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Scrub the data and write to CSV\n",
    "            for update in scrub(geojson=json.load(blob.data_stream)):\n",
    "                station_key = (city, provider, update[\"station\"])\n",
    "\n",
    "                # Skip unchanged data\n",
    "                if (\n",
    "                    (latest_update := latest_update_by_station.get(station_key))\n",
    "                    and latest_update[\"bikes\"] == update[\"bikes\"]\n",
    "                    and latest_update[\"stands\"] == update[\"stands\"]\n",
    "                ):\n",
    "                    skipped_updates_by_station[station_key] += 1\n",
    "                    continue\n",
    "\n",
    "                update[\"commit_at\"] = commit_at.isoformat()\n",
    "                update[\"skipped_updates\"] = skipped_updates_by_station[station_key]\n",
    "                writer.writerow(update)\n",
    "                latest_update_by_station[station_key] = update\n",
    "                skipped_updates_by_station[station_key] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897a7e2-140e-4ae3-9fea-ce636430a9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
